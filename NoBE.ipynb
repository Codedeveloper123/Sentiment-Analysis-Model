{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NoBE",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "14wj6iJMbpFP8cHdsI0nHfCYXTOgP73qc",
      "authorship_tag": "ABX9TyPpzdQQSlXJ0olWNDe1Dt57",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Codedeveloper123/Sentiment-Analysis-Model/blob/main/NoBE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOLgwpvFHpvu"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import seaborn as sns \n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer \n",
        "from sklearn.model_selection import train_test_split\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "import matplotlib.cm as cm\n",
        "from matplotlib import rcParams\n",
        "from collections import Counter\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import re\n",
        "import string\n",
        "from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "%matplotlib inline\n",
        " \n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from google.colab import drive "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25l_05JTH4ay",
        "outputId": "520045e8-073d-4503-bbc4-bdbf8cb9dbe3"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_u204IViWcEK"
      },
      "source": [
        "df = pd.read_csv('gdrive/My Drive/NOBE/Nobe.csv', encoding = \"latin-1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asSsalu2RF1o"
      },
      "source": [
        "df.columns = [\"label\", \"time\", \"date\", \"query\", \"username\", \"text\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krCEvZopR3C-"
      },
      "source": [
        "df['label'][df['label']==4]=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Khzsm642S9Uo"
      },
      "source": [
        "df['label'][df['label']==2]=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CKyCzaETGZ3"
      },
      "source": [
        "df['label'][df['label']==0]=-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XU5fgim3TLY5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "12a74b33-497a-4b80-c613-a86b2ac2f8eb"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>time</th>\n",
              "      <th>date</th>\n",
              "      <th>query</th>\n",
              "      <th>username</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1</td>\n",
              "      <td>1467811372</td>\n",
              "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>joy_wolf</td>\n",
              "      <td>@Kwesidei not the whole crew</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599994</th>\n",
              "      <td>1</td>\n",
              "      <td>2193601966</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>AmandaMarie1028</td>\n",
              "      <td>Just woke up. Having no school is the best fee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599995</th>\n",
              "      <td>1</td>\n",
              "      <td>2193601969</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>TheWDBoards</td>\n",
              "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599996</th>\n",
              "      <td>1</td>\n",
              "      <td>2193601991</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>bpbabe</td>\n",
              "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599997</th>\n",
              "      <td>1</td>\n",
              "      <td>2193602064</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>tinydiamondz</td>\n",
              "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599998</th>\n",
              "      <td>1</td>\n",
              "      <td>2193602129</td>\n",
              "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>RyanTrevMorris</td>\n",
              "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1599999 rows Ã— 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         label  ...                                               text\n",
              "0           -1  ...  is upset that he can't update his Facebook by ...\n",
              "1           -1  ...  @Kenichan I dived many times for the ball. Man...\n",
              "2           -1  ...    my whole body feels itchy and like its on fire \n",
              "3           -1  ...  @nationwideclass no, it's not behaving at all....\n",
              "4           -1  ...                      @Kwesidei not the whole crew \n",
              "...        ...  ...                                                ...\n",
              "1599994      1  ...  Just woke up. Having no school is the best fee...\n",
              "1599995      1  ...  TheWDB.com - Very cool to hear old Walt interv...\n",
              "1599996      1  ...  Are you ready for your MoJo Makeover? Ask me f...\n",
              "1599997      1  ...  Happy 38th Birthday to my boo of alll time!!! ...\n",
              "1599998      1  ...  happy #charitytuesday @theNSPCC @SparksCharity...\n",
              "\n",
              "[1599999 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeSRuL-xOB52",
        "outputId": "435cb72d-2e2f-43a7-8364-06b1f14a4dc1"
      },
      "source": [
        "df['text'] = df['text'].str.lower()\n",
        "df['text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0          is upset that he can't update his facebook by ...\n",
              "1          @kenichan i dived many times for the ball. man...\n",
              "2            my whole body feels itchy and like its on fire \n",
              "3          @nationwideclass no, it's not behaving at all....\n",
              "4                              @kwesidei not the whole crew \n",
              "                                 ...                        \n",
              "1599994    just woke up. having no school is the best fee...\n",
              "1599995    thewdb.com - very cool to hear old walt interv...\n",
              "1599996    are you ready for your mojo makeover? ask me f...\n",
              "1599997    happy 38th birthday to my boo of alll time!!! ...\n",
              "1599998    happy #charitytuesday @thenspcc @sparkscharity...\n",
              "Name: text, Length: 1599999, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGhcMhoXOYu_",
        "outputId": "faa01367-53c3-4a36-f3d5-f036c94111f3"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlqysfJhStV8",
        "outputId": "4a97e8ec-bbb8-4a49-858f-becb1d9fb6e3"
      },
      "source": [
        "stop_words = stopwords.words('english')\n",
        "stop_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SVYua_TRhZz"
      },
      "source": [
        " def clean_stop_words(text):\n",
        "  word_list = str(text).split()\n",
        "  cleaned_word_list = []\n",
        "  for word in word_list:\n",
        "    if word not in stop_words:\n",
        "      cleaned_word_list.append(word)\n",
        "  return \" \".join(cleaned_word_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yw35AevDTAVf"
      },
      "source": [
        "def clean_emails(text):\n",
        "  word_list = str(text).split()\n",
        "  cleaned_word_list = []\n",
        "  for word in word_list: \n",
        "    if not re.search(\".*@.*\",word):\n",
        "      cleaned_word_list.append(word)\n",
        "  return \" \".join(cleaned_word_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSQR92IdUt5v"
      },
      "source": [
        "english_punctuations = string.punctuation\n",
        "punctuations_list = english_punctuations\n",
        "def clean_punctuation(text):\n",
        "    translator = str.maketrans('', '', punctuations_list)\n",
        "    return text.translate(translator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBq7doR8Va11"
      },
      "source": [
        "def clean_repeating_char(text):\n",
        "    return re.sub(r'(.)\\1+', r'\\1', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCv5wiw53JBo"
      },
      "source": [
        "def clean_urls(data):\n",
        "    return re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVRbBHP13MZF"
      },
      "source": [
        "def clean_nums(data):\n",
        "    return re.sub('[0-9]+', '', data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPNkc5At3PCZ",
        "outputId": "fe149045-a8b7-4659-c157-5c194c6a91bd"
      },
      "source": [
        "print(\"Cleaning stop words...\")\n",
        "df['text'] = df['text'].apply(lambda tweet: clean_stop_words(tweet))\n",
        "print(\"Cleaning words with @...\")\n",
        "df['text'] = df['text'].apply(lambda tweet: clean_emails(tweet))\n",
        "print(\"Cleaning punctuation...\")\n",
        "df['text'] = df['text'].apply(lambda tweet: clean_punctuation(tweet))\n",
        "print(\"Cleaning urls...\")\n",
        "df['text'] = df['text'].apply(lambda tweet: clean_urls(tweet))\n",
        "print(\"Cleaning repeating characters...\")\n",
        "df['text'] = df['text'].apply(lambda tweet: clean_repeating_char(tweet))\n",
        "print(\"Cleaning numeric characters...\")\n",
        "df['text'] = df['text'].apply(lambda tweet: clean_nums(tweet))\n",
        "print(\"Done cleaning\")\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cleaning stop words...\n",
            "Cleaning words with @...\n",
            "Cleaning punctuation...\n",
            "Cleaning urls...\n",
            "Cleaning repeating characters...\n",
            "Cleaning numeric characters...\n",
            "Done cleaning\n",
            "   label  ...                                               text\n",
            "0     -1  ...  upset cant update facebok texting it might cry...\n",
            "1     -1  ...  dived many times bal managed save  rest go bounds\n",
            "2     -1  ...                    whole body fels itchy like fire\n",
            "3     -1  ...           no behaving al im mad here cant se there\n",
            "4     -1  ...                                         whole crew\n",
            "\n",
            "[5 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLDPcbVs5Dwt"
      },
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "df['text'] = df['text'].apply(tokenizer.tokenize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqWtjMVt5w6-",
        "outputId": "dab3cfc7-b0df-4a9a-cdf0-5d159e631a18"
      },
      "source": [
        "df.head()\n",
        "df['text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0          [upset, cant, update, facebok, texting, it, mi...\n",
              "1          [dived, many, times, bal, managed, save, rest,...\n",
              "2                     [whole, body, fels, itchy, like, fire]\n",
              "3          [no, behaving, al, im, mad, here, cant, se, th...\n",
              "4                                              [whole, crew]\n",
              "                                 ...                        \n",
              "1599994                [woke, up, schol, best, feling, ever]\n",
              "1599995    [thewdbcom, col, hear, old, walt, interviews, ...\n",
              "1599996                [ready, mojo, makeover, ask, details]\n",
              "1599997    [hapy, th, birthday, bo, al, time, tupac, amar...\n",
              "1599998                               [hapy, charitytuesday]\n",
              "Name: text, Length: 1599999, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX_--Zy-5-dQ"
      },
      "source": [
        "ps = PorterStemmer() \n",
        "def stemming_on_text(data):\n",
        "  text = [ps.stem(word) for word in data]\n",
        "  return text\n",
        "  df['text'] = df['text'].apply(lambda x: stemming_on_text(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDxU9WP36Gbi",
        "outputId": "9c57dae8-eff8-45e1-fb25-b59523fad852"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = nltk.WordNetLemmatizer()\n",
        "def lematzing_test(data):\n",
        "  text = [lemmatizer.lemmatize(word) for word in data]\n",
        "  return text\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: lematzing_test(x))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MkqLa3D8ptv"
      },
      "source": [
        "max_len = 500\n",
        "T= Tokenizer(num_words =2000)\n",
        "T.fit_on_texts(df['text'])\n",
        "value = T.texts_to_sequences(df['text'])\n",
        "j = sequence.pad_sequences(value,maxlen = max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49LFdaXWWGsJ"
      },
      "source": [
        "df['label'][df['label'] == 0] = 0.5\n",
        "df['label'][df['label'] == -1] = 0\n",
        "df['label'][df['label'] == 1] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyBQVDK16tV_"
      },
      "source": [
        "X_train,X_test,Y_train,Y_test=train_test_split(j,df['label'], test_size= 0.7, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTBH5X8SWeUG"
      },
      "source": [
        "model = Input(shape = [500]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7Gwznp1SaTY"
      },
      "source": [
        " model = Embedding(input_dim = 2000,output_dim=500,input_length=500)(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZc7ldqHTfCd"
      },
      "source": [
        "lstm = LSTM(64);\n",
        "model = lstm(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1XNwRGwVUyR"
      },
      "source": [
        "model = Dense(256)(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pu2G35qlVb3U"
      },
      "source": [
        "model = Activation('relu')(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2ozUxVOVqJM"
      },
      "source": [
        "model = Dropout(.5)(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FtBP6DZV4Cx"
      },
      "source": [
        "b = Activation('sigmoid')\n",
        "model = b(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpHt49A8YNNz"
      },
      "source": [
        "def nm_model():\n",
        "  model = Input(shape = [500]);\n",
        "  layer1 = model\n",
        "  model = Embedding(input_dim = 2000,output_dim=500,input_length=500)(model)\n",
        "  lstm = LSTM(64);\n",
        "  model = lstm(model)\n",
        "  model = Dense(256)(model)\n",
        "  model = Activation('relu')(model)\n",
        "  model = Dropout(.5)(model)\n",
        "  model = Dense(1)(model)\n",
        "  b = Activation('sigmoid')\n",
        "  model = b(model)\n",
        "  return Model(inputs = layer1,outputs = model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2ESZi0xYHLt",
        "outputId": "90abbb89-436f-4d0d-ab4f-86b2731ccfcc"
      },
      "source": [
        "model = nm_model()\n",
        "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy']) \n",
        "print(X_train)\n",
        "history=model.fit(X_train[:40000],Y_train[:40000],batch_size=100,epochs=6, validation_split=0.1)\n",
        "print('Training finished')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   0    0    0 ...  277  520  595]\n",
            " [   0    0    0 ...  191  193   68]\n",
            " [   0    0    0 ... 1748   54   12]\n",
            " ...\n",
            " [   0    0    0 ...   57   93  745]\n",
            " [   0    0    0 ...  346  931  146]\n",
            " [   0    0    0 ...    0    0   15]]\n",
            "Epoch 1/6\n",
            "360/360 [==============================] - 61s 80ms/step - loss: 0.5868 - accuracy: 0.6744 - val_loss: 0.5028 - val_accuracy: 0.7508\n",
            "Epoch 2/6\n",
            "360/360 [==============================] - 28s 79ms/step - loss: 0.4860 - accuracy: 0.7618 - val_loss: 0.4979 - val_accuracy: 0.7535\n",
            "Epoch 3/6\n",
            "360/360 [==============================] - 28s 79ms/step - loss: 0.4611 - accuracy: 0.7817 - val_loss: 0.4930 - val_accuracy: 0.7575\n",
            "Epoch 4/6\n",
            "360/360 [==============================] - 29s 80ms/step - loss: 0.4445 - accuracy: 0.7879 - val_loss: 0.5059 - val_accuracy: 0.7515\n",
            "Epoch 5/6\n",
            "360/360 [==============================] - 29s 80ms/step - loss: 0.4194 - accuracy: 0.7993 - val_loss: 0.5099 - val_accuracy: 0.7502\n",
            "Epoch 6/6\n",
            "360/360 [==============================] - 29s 79ms/step - loss: 0.4005 - accuracy: 0.8128 - val_loss: 0.5255 - val_accuracy: 0.7527\n",
            "Training finished\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}